{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, DataFrame, HiveContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "sqlContext_H = HiveContext(sc)\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "secure_rdd = sc.textFile(HOST + \"/users_info/SecureHealEmpTest.csv\").persist()\n",
    "first = secure_rdd.first()\n",
    "header = first.split(\"|\")\n",
    "row_data = secure_rdd.filter(lambda x: x != first).map( lambda x: x.split(\"|\") ) \\\n",
    "                        .map( lambda p: Row(**{header[i]:p[i] for i in range(len(header))}) ).persist()\n",
    "secure_rdd.unpersist() \n",
    "users = sqlContext_H.createDataFrame(row_data).select(\"Employee\", \"ProcDesc\", \"DeptName\", \"Dept\").persist()\n",
    "row_data.unpersist()\n",
    "sqlContext_H.registerDataFrameAsTable(users, \"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "users.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fields_list = ( \"ACCESS_DTTM\", \"USER_ID\", \"WORKSTATION\", \"ACCESS_WEEK\" )\n",
    "data = sqlContext_H.read.parquet( \n",
    "        HOST + \"/parquet3/08/*\"\n",
    "    ).select(*fields_list).persist()\n",
    "\n",
    "sqlContext_H.registerDataFrameAsTable(data, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_f = data.filter(\"ACCESS_WEEK = '31'\")\n",
    "sqlContext_H.registerDataFrameAsTable(data_f, 'data_f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = lambda table: \"\"\"\n",
    "    SELECT mainT1.USER_ID, mainT1.WORKSTATION, mainT1.TF * mainT2.IDF AS TF_IDF\n",
    "    FROM (SELECT T1.USER_ID, T1.WORKSTATION, T1.AMOUNT*1.0 / T2.TOTAL AS TF \n",
    "        FROM (SELECT USER_ID, WORKSTATION, COUNT(ACCESS_DTTM) AS AMOUNT\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID, WORKSTATION\n",
    "        ) AS T1\n",
    "        JOIN (SELECT USER_ID, COUNT(ACCESS_DTTM) AS TOTAL\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID\n",
    "        ) AS T2\n",
    "        ON T1.USER_ID = T2.USER_ID\n",
    "    ) AS mainT1 JOIN (\n",
    "        SELECT T.USER_ID, \n",
    "               log10( 1 + (SELECT COUNT(DISTINCT(USER_ID)) AS count FROM {table_name}) * 1.0 / COUNT(T.WORKSTATION) ) AS IDF\n",
    "        FROM (SELECT USER_ID, WORKSTATION\n",
    "            FROM {table_name}\n",
    "            GROUP BY USER_ID, WORKSTATION\n",
    "        ) AS T\n",
    "        GROUP BY T.USER_ID\n",
    "    ) AS mainT2 ON mainT1.USER_ID = mainT2.USER_ID\n",
    "\"\"\".format(table_name=table)\n",
    "\n",
    "df = sqlContext_H.sql(query(\"data_f\")).persist()\n",
    "sqlContext_H.registerDataFrameAsTable(df, 'df')\n",
    "\n",
    "query = lambda table: \" \".join([\n",
    "                \"SELECT t.USER_ID, t.WORKSTATION, t.TF_IDF\",\n",
    "                \", users.DeptName AS DEPT_NAME, users.Dept AS DEPT_ID, users.ProcDesc AS HOSPITAL\",\n",
    "                        \"FROM {table_name} AS t\",\n",
    "                        \"LEFT JOIN users ON t.USER_ID = users.Employee\",\n",
    "                    ]).format(table_name=table)\n",
    "\n",
    "res = sqlContext_H.sql(query('df')).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res.limit(5).toPandas()\n",
    "#res.filter(\"DEPT_ID IS NULL\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# One day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed = np.random.RandomState(seed=3)\n",
    "similarities = euclidean_distances(main1)\n",
    "\n",
    "mds = manifold.MDS(n_components=2, max_iter=300, eps=1e-9, random_state=seed,\n",
    "                   dissimilarity=\"precomputed\", n_jobs=1)\n",
    "pos = mds.fit(similarities).embedding_\n",
    "\n",
    "nmds = manifold.MDS(n_components=2, metric=False, max_iter=300, eps=1e-12,\n",
    "                    dissimilarity=\"precomputed\", random_state=seed, n_jobs=1,\n",
    "                    n_init=1)\n",
    "npos = nmds.fit_transform(similarities, init=pos)\n",
    "\n",
    "X_true = main1\n",
    "\n",
    "# Rescale the data\n",
    "pos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum())\n",
    "npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum())\n",
    "\n",
    "# Rotate the data\n",
    "clf = PCA(n_components=2)\n",
    "X_true = clf.fit_transform(X_true)\n",
    "\n",
    "pos = clf.fit_transform(pos)\n",
    "\n",
    "npos = clf.fit_transform(npos)\n",
    "\n",
    "#fig = plt.figure(1)\n",
    "fig = plt.figure(figsize=(14, 14))\n",
    "ax = plt.axes([0., 0., 1., 1.])\n",
    "\n",
    "s = 200\n",
    "\n",
    "max_ind = 0 \n",
    "for ind, val in enumerate(zip(npos[:, 0], npos[:, 1])):\n",
    "    for k, v in dict_with_dept.items():\n",
    "        if ind in v[-1] and k == None:\n",
    "            plt.scatter(val[0], val[1], color=colors[v[0]], s=s, lw=0, label=\"None\")\n",
    "\n",
    "\n",
    "similarities = similarities.max() / similarities * 100\n",
    "similarities[np.isinf(similarities)] = 0\n",
    "\n",
    "segments = [[X_true[i, :], X_true[j, :]]\n",
    "            for i in range(len(pos)) for j in range(len(pos))]\n",
    "values = np.abs(similarities)\n",
    "lc = LineCollection(segments,\n",
    "                    zorder=0, cmap=plt.cm.Blues,\n",
    "                    norm=plt.Normalize(0, values.max()))\n",
    "lc.set_array(similarities.flatten())\n",
    "lc.set_linewidths(0 * np.ones(len(segments)))\n",
    "ax.add_collection(lc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### 5-10\n",
    "seed = np.random.RandomState(seed=3)\n",
    "similarities = euclidean_distances(main1)\n",
    "\n",
    "mds = manifold.MDS(n_components=2, max_iter=300, eps=1e-9, random_state=seed,\n",
    "                   dissimilarity=\"precomputed\", n_jobs=1)\n",
    "pos = mds.fit(similarities).embedding_\n",
    "\n",
    "nmds = manifold.MDS(n_components=2, metric=False, max_iter=300, eps=1e-12,\n",
    "                    dissimilarity=\"precomputed\", random_state=seed, n_jobs=1,\n",
    "                    n_init=1)\n",
    "npos = nmds.fit_transform(similarities, init=pos)\n",
    "\n",
    "X_true = main1\n",
    "\n",
    "# Rescale the data\n",
    "pos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum())\n",
    "npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum())\n",
    "\n",
    "# Rotate the data\n",
    "clf = PCA(n_components=2)\n",
    "X_true = clf.fit_transform(X_true)\n",
    "\n",
    "pos = clf.fit_transform(pos)\n",
    "\n",
    "npos = clf.fit_transform(npos)\n",
    "\n",
    "#fig = plt.figure(1)\n",
    "fig = plt.figure(figsize=(14, 14))\n",
    "ax = plt.axes([0., 0., 1., 1.])\n",
    "\n",
    "s = 200\n",
    "max_ind = 0 \n",
    "lables_d = {}\n",
    "for ind, val in enumerate(zip(npos[:, 0], npos[:, 1])):\n",
    "    for k, v in dict_with_dept.items():\n",
    "        if ind in v[-1] and k != None and (5 <= len(v[-1]) <= 10):\n",
    "            #print(colors[v[0]], k)\n",
    "            lables_d[k] = str(ind) + '_plt'\n",
    "            globals()[str(ind) + '_plt'] = plt.scatter(val[0], val[1], color=colors[v[0]], s=s, lw=0, label=k)\n",
    "\n",
    "l_colors = []\n",
    "l_lable = []\n",
    "for ind_l, val_l in lables_d.items():\n",
    "    l_colors.append(globals()[val_l])\n",
    "    l_lable.append(ind_l)\n",
    "plt.legend(l_colors, l_lable, scatterpoints=1, loc='best', shadow=False)\n",
    "\n",
    "similarities = similarities.max() / similarities * 100\n",
    "similarities[np.isinf(similarities)] = 0\n",
    "\n",
    "\n",
    "segments = [[X_true[i, :], X_true[j, :]]\n",
    "            for i in range(len(pos)) for j in range(len(pos))]\n",
    "values = np.abs(similarities)\n",
    "lc = LineCollection(segments,\n",
    "                    zorder=0, cmap=plt.cm.Blues,\n",
    "                    norm=plt.Normalize(0, values.max()))\n",
    "lc.set_array(similarities.flatten())\n",
    "lc.set_linewidths(0 * np.ones(len(segments)))\n",
    "ax.add_collection(lc)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
